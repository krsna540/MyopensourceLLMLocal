{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNasLjYvXdF1K38XAURTGHu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OEPG2OBPKk3D","executionInfo":{"status":"ok","timestamp":1689166608959,"user_tz":-330,"elapsed":5,"user":{"displayName":"Krishna Chaitanya","userId":"08881280255229993027"}}},"outputs":[],"source":["class ChatSummaryPrompt:\n","    \"\"\"ChatSummaryPrompt -  Conversational summary to condense the chat history\n","    \"\"\"\n","    @staticmethod\n","    def generate_prompt(input_params: dict) -> str:\n","        \"\"\"generate_prompt - generating the prompt that will be executed by LLM model and can be customized as per user need.\n","\n","            Args:\n","                input_params (dict): dictinary of all required input parameters. Below are input_parameters :\n","                + query - user query for which response is expected\n","\n","            Returns:\n","                str: generated prompt that will be excuted by LLM model mentioned.\n","        \"\"\"\n","        template = \"\"\"Condense the following chat transcript by shortening and summarizing the content\n","                    without losing important information:\n","                    {query}\n","                    Condensed Transcript:\n","                    \"\"\"\n","        template = template.replace(\"{query}\", input_params[\"query\"])\n","        return template"]},{"cell_type":"code","source":["\n","class ContextQuestionAnsweringPrompt:\n","    \"\"\"ContextQuestionAnsweringPrompt -  Prompt to check whether a query can be answered with the provided context or not, reply will be \"yes\" or \"no\"\n","    \"\"\"\n","    @staticmethod\n","    def generate_prompt(input_params: dict) -> str:\n","        \"\"\"generate_prompt - generating the prompt that will be executed by LLM model and can be customized as per user need.\n","\n","            Args:\n","                input_params (dict): dictinary of all required input parameters. Below are input_parameters :\n","                + query - user query for which response is expected\n","                + context -  Prior information to be considered for prompt\n","\n","            Returns:\n","                str: generated prompt that will be excuted by LLM model mentioned.\n","        \"\"\"\n","        template = \"\"\"Does the following context contain the answer to the question?\n","                    Context: {context};\n","                    Question: {query};\n","                    Please answer yes or no! Answer:\n","                    \"\"\"\n","        template = template.replace(\"{context}\", input_params[\"context\"])\n","        template = template.replace(\"{query}\", input_params[\"query\"])\n","        return template\n","\n"],"metadata":{"id":"_7mWM8E7LXMF","executionInfo":{"status":"ok","timestamp":1689166645373,"user_tz":-330,"elapsed":5,"user":{"displayName":"Krishna Chaitanya","userId":"08881280255229993027"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Template_Name,Description,input_parameters\n","ChatSummaryPrompt,Conversational summary to condense the chat history,A python dictionary object  with following parameters- query: user chat history for which summary  is expected\n","ContextQuestionAnsweringPrompt,\"Prompt to check whether a query can be answered with the provided context or not, reply will be yes or no\",\"A python dictionary object  with following parameters- query: user query for which response is expected, context:Prior information to be considered for prompt\"\n","FewShotPrompt,prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance.,\"A python dictionary object  with following parameters- persona :a natural language portrayal of a specific individual, audience:to whom we are providing response, query:user query for which response is expected, max_length: maximum number of words, context\"\n","LanguageDetectionPrompt,\"In natural language processing models, Language detection prompting means detcting language based on inputs provided and responding with language name\",A python dictionary object  with following parameters- query:user query for which response is expected\n","SentimentAnalysisPrompt,\"Provide the sentiment (positive, negative or neutral) of a set of documents\",A python dictionary object  with following parameters- query:user query for which response is expected\n","SummarizationPrompt,Prompt to produce a summary for each document provided in query.,A python dictionary object  with following parameters- query:user query for which response is expected\n","TopicClassificationPrompt,Classify the given query into categories,\"A python dictionary object  with following parameters- query:user query for which response is expected, options: list of categories for classification\"\n","TranslationPrompt,Prompt to accept a target_language and translate each text provided to this target language.,\"A python dictionary object  with following parameters- query:user query for which response is expected,  target_language:Target language for translation\"\n","ZeroShotPrompt,\"In natural language processing models, zero-shot prompting means providing a prompt that is not part of the training data to the model, but the model can generate a result that you desire.\",\"A python dictionary object  with following parameters- persona :a natural language portrayal of a specific individual, audience:to whom we are providing response, query:user query for which response is expected, max_length: maximum number of words\""],"metadata":{"id":"OEOqRAApKsH_"}},{"cell_type":"code","source":[],"metadata":{"id":"un0aZoN9LY35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HKNWuJvmKssF"},"execution_count":null,"outputs":[]}]}